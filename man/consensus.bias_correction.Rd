% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/consensus.r
\name{consensus.bias_correction}
\alias{consensus.bias_correction}
\title{Jackknife bias correction for consensus Monte Carlo}
\usage{
consensus.bias_correction(
  alpha = 0.2,
  theta.w.pooled,
  theta,
  type = 2,
  return.pooled = FALSE,
  par.clust = NULL,
  forking = FALSE,
  ncores = 1,
  cov.tol = .Machine$double.eps
)
}
\arguments{
\item{alpha}{numeric between 0 and 1: the proportion of samples to use in 
the bias estimation.}

\item{a}{matrix of pooled, weighted samples from \code{consensus.weights}.}
}
\value{
A list with two elements:
\item{B}{The bias estimate.}
\item{theta.w.pooled}{A matrix of pooled, weighted and de-biased samples if 
\code{return.pooled} is \code{TRUE}.}
}
\description{
Computes the jackknife estimate of the small sample bias that arises in the 
consensus Monte Carlo algorithm. The result can be subtracted from the 
pooled samples output of \code{consensus.weights} to correct for this bias.
}
\details{
See \code{\link{consensus.weights}} for help on the arguments after
\code{alpha}.

Scott et al 2016 use \code{alpha = 0.2} in their examples.
}
\section{References}{

\itemize{
\item{Scott, Steven L., Blocker, A.W., Bonassi, F.V., Chipman, H.A., George, E.I. and McCulloch, R.E., 2016. Bayes and big data: The consensus Monte Carlo algorithm. \emph{International Journal of Management Science and Engineering Management}, 11(2), pp.78-88.}
}
}

\seealso{
\code{consensus.weights}
}
