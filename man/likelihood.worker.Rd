% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mopp.R
\name{likelihood.worker}
\alias{likelihood.worker}
\title{Compute all samples' log likelihoods using a single shard of data}
\usage{
likelihood.worker(df, context)
}
\arguments{
\item{df}{a matrix or data.frame of data. If using Spark the first column 
should contain integers identifying the partial posterior.}

\item{context}{a list of parameters common to all workers. See details.}
}
\value{
Matrix of log likelihoods for each sample in \code{context$theta}. 
If using Spark the first column will contain the index of the partial 
posterior from which the sample was drawn. The second column, or the only 
column if not using Spark, will contain the log likelihoods.
}
\description{
This is a worker function for computing sample log likelihoods when data are 
distributed. After having drawn samples from each partial posterior, they 
should be collated into a single matrix. Then this function can be applied 
to all shards of data in parallel, computing the log likelihoods for all 
samples, for each shard of data. This is the first step in the weight 
computation.
}
\details{
\code{context} should be a list with the following fields:
\describe{
\item{\code{theta}}{A matrix of all samples from the partial posteriors, 
pooled.}
\item{H}{Total number of samples (rows of \code{theta}).}
\item{\code{use_spark}}{Logical. \code{TRUE} if a Spark cluster is available.}
\item{\code{loglik.fun}}{The log likelihood function.}
\item{\code{args}}{A list of additional arguments to the log likelihood 
function. This list can be empty.}
}
}
