% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mopp.R
\name{mopp.weights}
\alias{mopp.weights}
\title{Calculate the MoPP algorithm sample importance weights}
\usage{
mopp.weights(
  x,
  theta,
  loglik = NULL,
  loglik.fun,
  type = 2,
  w.type_1 = NULL,
  keep.type1 = TRUE,
  keep.unnormalised = FALSE,
  return.loglik = FALSE,
  par.clust = NULL,
  ncores = 1,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{x}{either a list of matrices or a Spark table (class \code{tbl_spark}) 
containing the partitioned data. If a list, each element corresponds to a 
single part. If a Spark table, the first column of each element must be an 
integer vector identifying the part.}

\item{theta}{a list of matrices, each containing samples of model parameters 
from partial posterior distributions. Each matrix should have the same 
number of columns, which correspond to model parameters (including 
components of parameter vectors). Each list element corresponds to a single 
partial posterior.}

\item{loglik}{an optional list of numeric vectors, or single column 
matrices, whose values are log likelihoods. See details.}

\item{loglik.fun}{the log likelihood function. Optional if \code{loglik} is 
supplied. See details.}

\item{type}{an integer, either 1 or 2, specifying the weighting type to use.}

\item{w.type_1}{an optional list of single column matrices containing the 
unnormalised MoPP type 1 weights, the same as the output field. Supply this 
to speed up computation of the type 2 weights if the type 1 weights have 
already been computated.}

\item{keep.type1}{logical. If \code{TRUE} the type 1 weights are returned as 
well as the type 2 weights when \code{type = 2}. This is faster than 
computing the type 1 and type 2 weights separately.}

\item{keep.unnormalised}{logical. If \code{TRUE} the unnormalised weights 
are returned as well as the normalised.}

\item{return.loglik}{logical. If \code{TRUE} the log likelihoods are 
returned as an element of the output list. The format is the same as the 
\code{loglik} argument, with elements as matrices.}

\item{par.clust}{an optional cluster connection object from package 
\code{parallel}. Ignored if \code{x} is a Spark table.}

\item{ncores}{an optional integer specifying the number of CPU cores to use 
(see \code{\link[parallel]{makeCluster}}). The default, 1, signifies that 
\code{parallel} will not be used.}

\item{...}{additional parameters to be supplied to the log likelihood 
function. See details on \code{loglik.fun}.}
}
\value{
A list containing fields:
\item{Hvec}{A vector of the number of samples from each partial 
posterior.}
\item{wn.type_1 or wn.type_2}{The normalised weighted of type 1 or type 2, 
depending on the value of \code{type}.}
\item{wn.type_1}{Additionally returned if \code{keep.type1} is \code{TRUE}.}
\item{w.type_1}{The corresponding unnormalised weights, if 
\code{keep.unnormalised} is \code{TRUE}}
\item{w.type_2}{The corresponding unnormalised weights, if 
\code{keep.unnormalised} is \code{TRUE}}
\item{loglik}{log likelihoods, returned if \code{return.loglik} is 
\code{TRUE}}
Weights and log likelihoods are returned as lists of matrices with 1 column 
and rows corresponding to sample. The list elements correspond to partial 
posterior (same as argument \code{theta}).
}
\description{
Computes importance weights for the samples drawn from all partial posterior 
distributions to form a pooled, weighted sample that can be used to 
approximate expected values under the full data posterior distribution.
}
\details{
Data \code{x} can be held in local memory or distributed on a cluster. 
Distributed computation is performed using Spark interfaced with the 
\code{sparklyr} package. In this case \code{x} is a Spark table, with each 
part containing a matrix of data (a data "shard"). If \code{x} are held in 
local memory it should be a list of matrices. In either case the matrices 
must have the same number of columns but can have different numbers of rows.

Argument \code{loglik} is a list containing log likelihoods. It should have 
one element for each element of \code{x} (and \code{theta}), whose values 
are log likelihoods for all samples using just the data in the corresponding 
element of \code{x}. That is, each element of \code{loglik} contains log 
likelihoods for all samples in all elements of \code{theta}, so each element 
of \code{loglik} has the same number of rows, but computed using only a 
single data shard. This argument should be used when data shards are held by 
separate parties who are unable to communicate it to a single analyst, e.g. 
for privacy reasons. In this situation, all samples \code{theta} should be 
distributed amongst the data owners, who should compute the log likelihoods 
separately and send those to the analyst.

Argument \code{loglik.fun} is a function that returns the log likelihood of 
parameter samples. It should have two arguments and the \code{...} argument. 
The first argument is a matrix of parameter samples in the same form as the 
elements of \code{theta}. The second argument is a matrix of data in the 
same form as the elements of \code{x}. The \code{...} should follow, and can 
be used in the definition of \code{loglik.fun} using the \code{list(...)} 
constuct. This allows additional named arguments to be supplied to the 
likelihood function (e.g. additional parameters). The returned value is a 
vector of log likelihoods with one value for each row of the first argument.

Parameter samples \code{theta} should be sampled from the partial posterior 
distributions using proper priors rather than the "fractionated" priors of 
the consensus Monte Carlo algorithm of Scott et al 2016.
}
\section{References}{

\itemize{
\item{Scott, Steven L., Blocker, A.W., Bonassi, F.V., Chipman, H.A., George, E.I. and McCulloch, R.E., 2016. Bayes and big data: The consensus Monte Carlo algorithm. \emph{International Journal of Management Science and Engineering Management}, 11(2), pp.78-88.}
}
}

