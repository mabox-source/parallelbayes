% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mopp.R
\name{mopp.weights}
\alias{mopp.weights}
\title{Calculate the MoPP algorithm sample importance weights}
\usage{
mopp.weights(
  x,
  theta,
  loglik = NULL,
  loglik.fun = NULL,
  type = 1,
  subsample_size = NULL,
  laplace.type_1 = FALSE,
  laplace.type_2 = FALSE,
  laplace.type_3 = FALSE,
  laplace.type_2.sample_size = NULL,
  laplace.type_3.sample_size = NULL,
  laplace.type_3.scale = NULL,
  laplace.type_3.dof = NULL,
  params = NULL,
  w.type_1 = NULL,
  keep.type1 = TRUE,
  keep.unnormalised = FALSE,
  return.loglik = FALSE,
  par.clust = NULL,
  forking = FALSE,
  ncores = 1,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{x}{either a list of matrices or a Spark table (class \code{tbl_spark}) 
containing the partitioned data. If a list, each element corresponds to a 
single part. If a Spark table, the first column of each element must be an 
integer vector identifying the part.}

\item{theta}{a list of matrices, each containing samples of model parameters 
from partial posterior distributions. Each matrix should have the same 
number of columns, which correspond to model parameters (including 
components of parameter vectors). Each list element corresponds to a single 
partial posterior.}

\item{loglik}{an optional list of numeric vectors, or single column 
matrices, whose values are log likelihoods. See details.}

\item{loglik.fun}{the log likelihood function. Optional if \code{loglik} is 
supplied. See details.}

\item{type}{an integer, either 1, 2 or 3, specifying the weighting type to 
use.}

\item{laplace.type_3.scale}{an optional matrix}

\item{laplace.type_3.dof}{an optional numeric. Must be greater than the dimension of the model plus 1.}

\item{w.type_1}{an optional list of single column matrices containing the 
unnormalised MoPP type 1 weights, the same as the output field. Supply this 
to speed up computation of the type 2 weights if the type 1 weights have 
already been computated.}

\item{keep.type1}{logical. If \code{TRUE} the type 1 weights are returned as 
well as the type 2/3 weights when \code{type = 2} or \code{type = 3}. This 
is faster than computing the different weights separately.}

\item{keep.unnormalised}{logical. If \code{TRUE} the unnormalised weights 
are returned as well as the normalised.}

\item{return.loglik}{logical. If \code{TRUE} the log likelihoods are 
returned as an element of the output list. The format is the same as the 
\code{loglik} argument, with elements as matrices.}

\item{par.clust}{an optional cluster connection object from package 
\code{parallel}. Ignored if \code{x} is a Spark table.}

\item{forking}{logical. If \code{TRUE}, use forking functions 
\code{\link[parallel]{mclapply}}, \code{\link[parallel]{mcmapply}}. Does not 
work on Windows.}

\item{ncores}{an optional integer specifying the number of CPU cores to use 
(see \code{\link[parallel]{makeCluster}}). The default, 1, signifies that 
\code{parallel} will not be used.}

\item{...}{additional parameters to be supplied to the log likelihood 
function. See details on \code{loglik.fun}.}
}
\value{
A list containing fields:
\item{Hvec}{A vector of the number of samples from each partial 
posterior.}
\item{wn.type_1, wn.type_2 or wn.type_3}{The normalised weighted of type 1, 
2 or type 3, depending on the value of \code{type}.}
\item{wn.type_1}{Additionally returned if \code{keep.type1} is \code{TRUE}.}
\item{w.type_1}{The corresponding unnormalised weights, if 
\code{keep.unnormalised} is \code{TRUE} and \code{type = 1}}
\item{w.type_2}{The corresponding unnormalised weights, if 
\code{keep.unnormalised} is \code{TRUE} and \code{type = 2}}
\item{w.type_3}{The corresponding unnormalised weights, if 
\code{keep.unnormalised} is \code{TRUE} and \code{type = 3}}
\item{loglik}{log likelihoods, returned if \code{return.loglik} is 
\code{TRUE}}
\item{subsamples}{List of samples, like \code{theta}, after taking a 
subsample in the \code{type = 3} algorithm}
\item{subsample.inds}{List of sample indices indicating which were taken in 
the subsample in the \code{type = 3} algorithm}
\item{kl_hat}{Estimates of the KL divergence from the posterior distribution 
to each of the partial posteriors. Only returned if \code{type = 3}}
Weights and log likelihoods are returned as lists of matrices with 1 column 
and rows corresponding to sample. The list elements correspond to partial 
posterior (same as argument \code{theta}).
}
\description{
Computes importance weights for the samples drawn from all partial posterior 
distributions to form a pooled, weighted sample that can be used to 
approximate expected values under the full data posterior distribution.
}
\details{
Data \code{x} can be held in local memory or distributed on a cluster. 
Distributed computation is performed using Spark interfaced with the 
\code{sparklyr} package. In this case \code{x} is a Spark table, with each 
part containing a matrix of data (a data "shard"). If \code{x} are held in 
local memory it should be a list of matrices. In either case the matrices 
must have the same number of columns but can have different numbers of rows.

Argument \code{loglik} is a list containing log likelihoods. It should have 
one element for each element of \code{x} (and \code{theta}), whose values 
are log likelihoods for all samples using just the data in the corresponding 
element of \code{x}. That is, each element of \code{loglik} contains log 
likelihoods for all samples in all elements of \code{theta}, so each element 
of \code{loglik} has the same number of rows, but computed using only a 
single data shard. This argument should be used when data shards are held by 
separate parties who are unable to communicate it to a single analyst, e.g. 
for privacy reasons. In this situation, all samples \code{theta} should be 
distributed amongst the data owners, who should compute the log likelihoods 
separately and send those to the analyst.

Argument \code{loglik.fun} is a function that returns the log likelihood of 
parameter samples. It should have two arguments and the \code{...} argument. 
The first argument is a matrix of parameter samples in the same form as the 
elements of \code{theta}. The second argument is a matrix of data in the 
same form as the elements of \code{x}. The \code{...} should follow, and can 
be used in the definition of \code{loglik.fun} using the \code{list(...)} 
constuct. This allows additional named arguments to be supplied to the 
likelihood function (e.g. additional parameters). The returned value is a 
vector of log likelihoods with one value for each row of the first argument.

Parameter samples \code{theta} should be sampled from the partial posterior 
distributions using proper priors rather than the "fractionated" priors of 
the consensus Monte Carlo algorithm of Scott et al 2016.
}
\section{Reusing output from \code{type == 1}}{


Some of the computational steps are common to the three versions of the 
algorithm. Therefore it can save computation to reuse the intermediate 
results when calling with \code{type == 2} or \code{type == 3} after an 
initial call with \code{type == 1}.

If Laplace approximations were used in the initial run, the samples from 
those approximations should be supplied in elements of \code{theta}, 
appended after the samples from the partial posteriors. Arguments 
\code{laplace.type_1}, \code{laplace.type_2} and \code{laplace.type_3} 
should all be \code{FALSE}.
}

\section{References}{

\itemize{
\item{Scott, Steven L., Blocker, A.W., Bonassi, F.V., Chipman, H.A., George, E.I. and McCulloch, R.E., 2016. Bayes and big data: The consensus Monte Carlo algorithm. \emph{International Journal of Management Science and Engineering Management}, 11(2), pp.78-88.}
}
}

