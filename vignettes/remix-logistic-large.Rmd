---
title: "Remix in Large Logistic Regression Examples"
author: "Marc Box"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: "references.bib"
vignette: >
  %\VignetteIndexEntry{Remix in Large Logistic Regression Examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Remix in Large Logistic Regression Examples

In this vignette we simulate artificial data sets for logistic regression using 
a similar approach to Neiswanger et al 2013. This is parameterised in the 
following way:

* $50,000$ observations
* $50$ predictor variables
* Predictor variables follow independent standard normal distributions
* Regression coefficients follow independent standard normal distributions
* No regression intercept.

```{r}
n <- 50000
p <- 50

coefs <- rnorm(p)
x <- matrix(rnorm(n * p), n, p)
eta <- x %*% coefs
y <- apply(cbind(1, exp(eta)), MARGIN = 1, FUN = function(p) {sample(0:1, 1, prob = p)})
```

In the normal data experiments we were able to measure accuracy from the true 
posterior statistics (mean and covariance). In logistic regression the true 
posterior is unknown and can only be approximated (here using MCMC).

We will therefore measure accuracy from the "vanilla" ($M = 1$) MCMC estimates.

## Sample from full posterior

We place an uncorrelated multivariate normal prior on coefficients. The mean is 
$0$ and the variances are $2.5^2$. In the fractionated prior, the variances 
become $(2.5 M) ^2$.

Use a large sample size `H` here: we want to get as good convergence with the 
"vanilla" sample as possible, to use it as a benchmark. We will discard 50% of 
samples as a burn-in.

```{r}
H <- 2000000
burnin <- floor(0.5 * H)

params <- list()
params$mu <- rep(0, 5)
params$Sigma <- 2.5 ^ 2 * diag(5)

out <- logistic.sampler(
  y = y,
  x = x,
  H = H,
  params = params
)

samples.full <- out$samples[(burnin + 1):H,]
```

## Distributed sampling from partial posteriors

Randomly partition the data into $M = 4$ parts.

```{r}
M <- 4
x.list <- partition(cbind(y, x), M, random = TRUE, balanced = TRUE)
print(sapply(x.list, nrow))
```

Connect to Spark cluster (localhost) and distribute the parts.

```{r}
library(sparklyr)

config <- spark_config()
config$spark.executor.memory <- "1125m"
config$spark.executor.cores <- 1
config$spark.executor.instances <- 2
config$spark.dynamicAllocation.enabled <- FALSE

config$`sparklyr.cores.local` <- 8
config$`sparklyr.shell.driver-memory` <- "8G"
config$spark.dynamicAllocation.enabled <- "false"
config$spark.executor.instances <- M

sc <- spark_connect(
  master = "local",
  config = config
)

invoke(spark_context(sc), "getExecutorMemoryStatus")

x.cluster.parts <- list()
for (p in 1:M) x.cluster.parts[[p]] <- sdf_copy_to(sc, as.data.frame(cbind(p, x.list[[p]])), repartition = 1, overwrite = TRUE)
# This creates a Spark table with M "partitions" (i.e. distributed parts).
x.cluster <- do.call(sdf_bind_rows, x.cluster.parts)

# Check a few things.
spark_apply(x.cluster, function(p) nrow(p), names = "n")
spark_apply(x.cluster, function(p) tail(p, 2))

# Check number of partitions.
sdf_num_partitions(x.cluster)
```

Define the function to distribute. We will use `context` to supply the prior 
parameters, as well as the sample size and the random seeds.

```{r}
f <- function(df, context) {
  # The partition label.
  part <- df[1,1]

  set.seed(context$seeds[part])

  out <- logistic.sampler(
    y = df[,1],
    x = df[,-1],
    H = context$H,
    params = context
  )

  cbind(part, out$samples[(context$burnin + 1):context$H,])
}
```

Now run. We will set discard 50% of samples as a burn-in.

```{r}
# Set master seed.
set.seed(1910161352)
H <- 400000
burnin <- 0.5 * H.
# Seeds for each executor.
# Default RNG algorithm (Mersenne-Twister) uses 32 bit integers.
seeds <- sample.int(2^32 - 1, M) - 2^32 / 2

# Context object sent to executors.
ctx <- list(
  H = H,
  burnin = burnin,
  mu = params$mu,
  Sigma = params$Sigma,
  seeds = seeds
)

res.cluster <- spark_apply(x.cluster, f, context = ctx)

# Import results to local memory and convert to a list.
res <- as.data.frame(res.cluster)
samples.part <- split(res[,-1], res[,1])
samples.part <- lapply(samples.part, FUN = as.matrix)
```

This time using the fractionated prior:

```{r}
# Seeds for each executor.
# Default RNG algorithm (Mersenne-Twister) uses 32 bit integers.
seeds <- sample.int(2^32 - 1, M) - 2^32 / 2

# Context object sent to executors.
ctx <- list(
  H = H,
  burnin = burnin,
  mu = params$mu,
  Sigma = params$Sigma * M ^ 2,
  seeds = seeds
)

res.cluster <- spark_apply(x.cluster, f, context = ctx)

# Import results to local memory and convert to a list.
res <- as.data.frame(res.cluster)
samples.part.frac <- split(res[,-1], res[,1])
samples.part.frac <- lapply(samples.part.frac, FUN = as.matrix)
```


## Pooling algorithms

Here we define the log likelihood function, to be used by remix. This is 
specific to the model with a binary outcome.

```{r}
loglik.fun <- function(theta, x, mem.limit = 1024 ^ 3) {
  n <- nrow(x)
  H <- nrow(theta)
  ll <- matrix(NA, H, 1)
  mem.req <- 2 * n * H * 8
  n_chunks <- min(ceiling(mem.req / mem.limit), H)
  Hk <- ceiling(H / n_chunks)
  for (k in 1:n_chunks) {
    if ((k - 1) * Hk + 1 > H) break
    inds <- ((k - 1) * Hk + 1):min(k * Hk, H)
    eta <- tcrossprod(x[,-1], theta[inds,,drop = FALSE])
    ll[inds,] <- .colSums(
      x[,1] * eta - lrowsums(abind::abind(eta, matrix(0, n, length(inds)), along = 3), 3)
      n,
      length(inds)
    )
  }
}
```

Remix.

```{r}
timer <- proc.time()
remix.out <- remix.weights(
  x.cluster,
  samples.part,
  loglik.fun = loglik.fun,
  type = 2,
  keep.type1 = TRUE,
  Sigma = Sigma,
  d = d,
  mem.limit = 1024 ^ 3
)
print(proc.time() - timer)
```

Consensus Monte Carlo algorithm.

```{r}
timer <- proc.time()
consensus1.out <- consensus.weights(
  samples.part.frac,
  type = 1,
  return.pooled = TRUE
)
print(proc.time() - timer)

timer <- proc.time()
consensus2.out <- consensus.weights(
  samples.part.frac,
  type = 2,
  return.pooled = TRUE
)
print(proc.time() - timer)
```

Density product estimators.

```{r}
timer <- proc.time()
ndpe.out <- dpe.master(
  samples.part.frac,
  type = "ndpe",
  keep.type1 = TRUE
)
print(proc.time() - timer)

timer <- proc.time()
sdpe.out <- dpe.master(
  samples.part.frac,
  type = "sdpe",
  keep.type1 = TRUE
)
print(proc.time() - timer)
```


### Posterior parameter estimates

Here we compute the pooling algorithm estimates of the posterior mean, median, 
2.5% quantile, 97.5% quantile and standard deviation of each model parameter 
dimension.

We also estimate the effective sample size (ESS). "Roughly speaking, the ESS of 
a quantity of interest captures how many independent draws contain the same 
amount of information as the dependent sample obtained by the MCMC algorithm" 
(Vehtari et al 2019). The maximum possible for the consensus and DPE algorithms 
is $\frac{H^{total}}{M}$; for the other methods, including remix, the maximum 
possible is $H$.

```{r}
posterior.estimates <- list()
ess <- sapply(c("Vanilla", "Naive", "Remix 1", "Remix 2", "Consensus 1", "Consensus 2", "NDPE", "SDPE"), FUN = function(a) {NA})
```

Full posterior sampling.

```{r}
posterior.estimates[["Vanilla"]] <- apply(
  samples.full,
  2,
  FUN = function(theta) {
    c(
      "Mean" = mean(theta),
      "Median" = median(theta),
      "2.5%" = quantile(theta, 0.025),
      "97.5%" = quantile(theta, 0.975),
      "Std. dev." = sd(theta)
    )
  }
)

ess["Vanilla"] <- mcmcse::multiESS(samples.full)
```

Naive pooling of partial posterior samples.

```{r}
posterior.estimates[["Naive"]] <- apply(
  abind::abind(samples.part, along = 1),
  2,
  FUN = function(theta) {
    c(
      "Mean" = mean(theta),
      "Median" = median(theta),
      "2.5%" = quantile(theta, 0.025),
      "97.5%" = quantile(theta, 0.975),
      "Std. dev." = sd(theta)
    )
  }
)

ess["Naive"] <- mcmcse::multiESS(abind::abind(samples.part, along = 1))
```

Remix algorithm 1.

```{r}
posterior.estimates[["Remix 1"]] <- rbind(
  "Mean" = remix.mean(samples.part, remix.out$wn.type_1, type = 1, Hvec = remix.out$Hvec),
  "Median" = remix.quantile(samples.part, 0.5, remix.out$wn.type_1, type = 1, Hvec = remix.out$Hvec)["root",],
  "2.5%" = remix.quantile(samples.part, 0.025, remix.out$wn.type_1, type = 1, Hvec = remix.out$Hvec)["root",],
  "97.5%" = remix.quantile(samples.part, 0.975, remix.out$wn.type_1, type = 1, Hvec = remix.out$Hvec)["root",]
)
posterior.estimates[["Remix 1"]] <- rbind(
  posterior.estimates[["Remix 1"]],
  "Std. Dev" = sqrt(remix.mean(lapply(samples.part, FUN = function(theta) {theta ^ 2}), remix.out$wn.type_1, type = 1, Hvec = remix.out$Hvec) - posterior.estimates[["Remix 1"]] ^ 2)
)

ess["Remix 1"] <- exp(2 * log(sum(remix.out$Hvec)) -
  lrowsums(
    unlist(
      mapply(
        FUN = function(w, h) {2 * w + 2 * log(h)},
        remix.out$wn.type_1,
        remix.out$Hvec,
        SIMPLIFY = FALSE
      )
    )
  )
)
```

Remix algorithm 2.

```{r}
posterior.estimates[["Remix 2"]] <- rbind(
  "Mean" = remix.mean(samples.part, remix.out$wn.type_2, type = 2, Hvec = remix.out$Hvec),
  "Median" = remix.quantile(samples.part, 0.5, remix.out$wn.type_2, type = 2, Hvec = remix.out$Hvec)["root",],
  "2.5%" = remix.quantile(samples.part, 0.025, remix.out$wn.type_2, type = 2, Hvec = remix.out$Hvec)["root",],
  "97.5%" = remix.quantile(samples.part, 0.975, remix.out$wn.type_2, type = 2, Hvec = remix.out$Hvec)["root",]
)
posterior.estimates[["Remix 2"]] <- rbind(
  posterior.estimates[["Remix 2"]],
  "Std. Dev" = sqrt(remix.mean(lapply(samples.part, FUN = function(theta) {theta ^ 2}), remix.out$wn.type_2, type = 2, Hvec = remix.out$Hvec) - posterior.estimates[["Remix 2"]] ^ 2)
)

ess["Remix 2"] <- exp(-lrowsums(2 * unlist(remix.out$wn.type_2)))
```

Consensus Monte Carlo.

```{r}
posterior.estimates[["Consensus 1"]] <- apply(
  consensus.out$theta.w.pooled,
  2,
  fun = function(theta) {
    c(
      "mean" = mean(theta),
      "median" = median(theta),
      "2.5%" = quantile(theta, 0.025),
      "97.5%" = quantile(theta, 0.975),
      "std. dev." = sd(theta)
    )
  }
)

ess["Consensus 1"] <- mcmcse::multiESS(consensus1.out$theta.w.pooled)

posterior.estimates[["Consensus 2"]] <- apply(
  consensus.out$theta.w.pooled,
  2,
  fun = function(theta) {
    c(
      "mean" = mean(theta),
      "median" = median(theta),
      "2.5%" = quantile(theta, 0.025),
      "97.5%" = quantile(theta, 0.975),
      "std. dev." = sd(theta)
    )
  }
)

ess["Consensus 2"] <- mcmcse::multiESS(consensus2.out$theta.w.pooled)
```

Density product estimators.

```{r}
posterior.estimates[["NDPE"]] <- apply(
  ndpe.out$theta.pooled,
  2,
  fun = function(theta) {
    c(
      "mean" = mean(theta),
      "median" = median(theta),
      "2.5%" = quantile(theta, 0.025),
      "97.5%" = quantile(theta, 0.975),
      "std. dev." = sd(theta)
    )
  }
)

ess["NDPE"] <- mcmcse::multiESS(ndpe.out$theta.pooled)

posterior.estimates[["SDPE"]] <- apply(
  sdpe.out$theta.pooled,
  2,
  fun = function(theta) {
    c(
      "mean" = mean(theta),
      "median" = median(theta),
      "2.5%" = quantile(theta, 0.025),
      "97.5%" = quantile(theta, 0.975),
      "std. dev." = sd(theta)
    )
  }
)

ess["SDPE"] <- mcmcse::multiESS(sdpe.out$theta.pooled)
```


Quick look at error from vanilla MCMC estimates:

```{r}
sweep(
  sapply(
    posterior.estimates, FUN = function(p) {p["Mean",]}
  ),
  MARGIN = 1,
  STATS = posterior.estimates[["Vanilla"]]["Mean",],
  FUN = "-"
)
```


Close cluster.

```{r}
spark_disconnect(sc)
```
